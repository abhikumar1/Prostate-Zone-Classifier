# -*- coding: utf-8 -*-
"""ProstateX_Segmentation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/186RBni34J_3To7Zr-6EJt0v9hq0Mfusk
"""

# Commented out IPython magic to ensure Python compatibility.
# from google.colab import drive
# drive.mount('/content/drive')

FOLDERNAME = ''
# assert FOLDERNAME is not None, "[!] Enter the foldername."

import sys
sys.path.append('/{}'.format(FOLDERNAME))

# %cd /content/drive/Shareddrives/CS231N project/

# !pip install pydicom
# !jupyter nbextension enable --py widgetsnbextension

import torch
from torch.utils.data import Dataset
import pydicom
import os
import glob
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import ipywidgets as widgets
from torch import nn
from torchsummary import summary
import time

print(torch.__version__)

import sys

print(sys.version)

class ProstateXDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.patient_dirs = sorted([
            os.path.join(root_dir, 'ProstateX_orig', 'manifest-1605042674814', 'PROSTATEx', d)
            for d in os.listdir(os.path.join(root_dir, 'ProstateX_orig', 'manifest-1605042674814', 'PROSTATEx'))
            if os.path.isdir(os.path.join(root_dir, 'ProstateX_orig', 'manifest-1605042674814', 'PROSTATEx', d))
        ])

        self.idx_to_class = {
            0: 'peripheral zone',
            1: 'transition zone'
        }

    def __len__(self):
        return len(self.patient_dirs)

    def __getitem__(self, idx):
        patient_dir = self.patient_dirs[idx]

        # MRI
        dir1_mri = next(d for d in os.listdir(patient_dir) if not d.startswith('.'))
        dir2_mri = next(d for d in os.listdir(os.path.join(patient_dir, dir1_mri)) if not d.startswith('.'))
        mri_path = os.path.join(patient_dir, dir1_mri, dir2_mri)
        mri_images = sorted(glob.glob(os.path.join(mri_path, '*.dcm')), key=lambda x: int(x.split('/')[-1].split('-')[1].split('.')[0]))
        mri_images = np.stack([pydicom.dcmread(img).pixel_array.astype(np.float32) for img in mri_images])

        # Segmentation
        seg_dir = patient_dir.replace('ProstateX_orig', 'ProstateX_seg')
        dir1_seg = next(d for d in os.listdir(seg_dir) if not d.startswith('.'))
        dir2_seg = next(d for d in os.listdir(os.path.join(seg_dir, dir1_seg)) if not d.startswith('.'))
        seg_path = os.path.join(seg_dir, dir1_seg, dir2_seg, '1-1.dcm')
        seg_image = pydicom.dcmread(seg_path).pixel_array.astype(np.float32)

        # Reshape the segmentation to [4, num_slices, H, W] (Assume seg_image is (4*num_slices, H, W)
        num_slices = mri_images.shape[0]
        seg_image = seg_image.reshape(4, num_slices, *seg_image.shape[1:])
        # seg_image = seg_image[:len(self.idx_to_class)] # Slice out the first two classes (prostate & transition zones)
        seg_image[2] = 1 - seg_image[0] - seg_image[1]
        seg_image = seg_image[:3] # Slice out the first two classes (prostate & transition zones)


        mri_tensor = torch.tensor(mri_images, dtype=torch.float32)
        seg_tensor = torch.tensor(seg_image, dtype=torch.long)
        if self.transform:
            mri_tensor = self.transform(mri_tensor)
            seg_tensor = self.transform(seg_tensor)
        return mri_tensor, seg_tensor

dataset = ProstateXDataset(root_dir='./Data/PROSTATEx')

# mri, seg = dataset[0]
# print("MRI shape:", mri.shape)
# print("Segmentation shape:", seg.shape)

# # VISUALIZE A DATAPOINT
# def overlay_segmentations(slice_idx):
#     colors = [
#         (1, 0, 0, 0.5),
#         (0, 1, 0, 0.5),
#     ]

#     fig, axs = plt.subplots(1, 2, figsize=(10, 5))
#     axs = axs.flatten()

#     mri_np = mri[slice_idx]
#     seg_np = seg[:, slice_idx]

#     for i, ax in enumerate(axs):
#         ax.imshow(mri_np, cmap='gray')
#         mask = np.ma.masked_where(seg_np[i] == 0, seg_np[i])
#         ax.imshow(mask, cmap=matplotlib.colors.ListedColormap([colors[i]]), interpolation='none', alpha=0.5)
#         ax.set_title(f'{dataset.idx_to_class[i]}')
#         ax.axis('off')

#     plt.suptitle(f'Slice Index: {slice_idx}')
#     plt.tight_layout(rect=[0, 0, 1, 0.95])
#     plt.show()

# slider = widgets.IntSlider(
#     value=0,
#     min=0,
#     max=mri.shape[0] - 1,
#     step=1,
#     description='Slice Index',
#     continuous_update=False
# )
# widgets.interactive(overlay_segmentations, slice_idx=slider)

"""Github link for the 3D Unet implementation:
https://github.com/jphdotam/Unet3D

Original Unet paper: https://arxiv.org/pdf/1505.04597

"""

"""Adapted from https://github.com/milesial/Pytorch-UNet/tree/master/unet"""
import torch
import torch.nn as nn
import torch.nn.functional as F


class UNet(nn.Module):
    def __init__(self, n_channels, n_classes, width_multiplier=1, trilinear=True, use_ds_conv=False):
        """A simple 3D Unet, adapted from a 2D Unet from https://github.com/milesial/Pytorch-UNet/tree/master/unet
        Arguments:
          n_channels = number of input channels; 3 for RGB, 1 for grayscale input
          n_classes = number of output channels/classes
          width_multiplier = how much 'wider' your UNet should be compared with a standard UNet
                  default is 1;, meaning 32 -> 64 -> 128 -> 256 -> 512 -> 256 -> 128 -> 64 -> 32
                  higher values increase the number of kernels pay layer, by that factor
          trilinear = use trilinear interpolation to upsample; if false, 3D convtranspose layers will be used instead
          use_ds_conv = if True, we use depthwise-separable convolutional layers. in my experience, this is of little help. This
                  appears to be because with 3D data, the vast vast majority of GPU RAM is the input data/labels, not the params, so little
                  VRAM is saved by using ds_conv, and yet performance suffers."""
        super(UNet, self).__init__()
        _channels = (32, 64, 128, 256, 512)
        self.n_channels = n_channels
        self.n_classes = n_classes
        self.channels = [int(c*width_multiplier) for c in _channels]
        self.trilinear = trilinear
        self.convtype = DepthwiseSeparableConv3d if use_ds_conv else nn.Conv3d

        self.inc = DoubleConv(n_channels, self.channels[0], conv_type=self.convtype)
        self.down1 = Down(self.channels[0], self.channels[1], conv_type=self.convtype)
        self.down2 = Down(self.channels[1], self.channels[2], conv_type=self.convtype)
        self.down3 = Down(self.channels[2], self.channels[3], conv_type=self.convtype)
        factor = 2 if trilinear else 1
        self.down4 = Down(self.channels[3], self.channels[4] // factor, conv_type=self.convtype)
        self.up1 = Up(self.channels[4], self.channels[3] // factor, trilinear)
        self.up2 = Up(self.channels[3], self.channels[2] // factor, trilinear)
        self.up3 = Up(self.channels[2], self.channels[1] // factor, trilinear)
        self.up4 = Up(self.channels[1], self.channels[0], trilinear)
        self.outc = OutConv(self.channels[0], n_classes)

    def forward(self, x):
        x1 = self.inc(x)
        x2 = self.down1(x1)
        x3 = self.down2(x2)
        x4 = self.down3(x3)
        x5 = self.down4(x4)
        x = self.up1(x5, x4)
        x = self.up2(x, x3)
        x = self.up3(x, x2)
        x = self.up4(x, x1)
        logits = self.outc(x)
        return logits


class DoubleConv(nn.Module):
    """(convolution => [BN] => ReLU) * 2"""

    def __init__(self, in_channels, out_channels, conv_type=nn.Conv3d, mid_channels=None):
        super().__init__()
        if not mid_channels:
            mid_channels = out_channels
        self.double_conv = nn.Sequential(
            conv_type(in_channels, mid_channels, kernel_size=3, padding=1),
            # nn.BatchNorm3d(mid_channels),
            nn.InstanceNorm3d(mid_channels),
            nn.ReLU(inplace=True),
            conv_type(mid_channels, out_channels, kernel_size=3, padding=1),
            nn.InstanceNorm3d(mid_channels),
            # nn.BatchNorm3d(out_channels),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        return self.double_conv(x)


class Down(nn.Module):
    """Downscaling with maxpool then double conv"""

    def __init__(self, in_channels, out_channels, conv_type=nn.Conv3d):
        super().__init__()
        self.maxpool_conv = nn.Sequential(
            nn.MaxPool3d(2),
            DoubleConv(in_channels, out_channels, conv_type=conv_type)
        )

    def forward(self, x):
        return self.maxpool_conv(x)


class Up(nn.Module):
    """Upscaling then double conv"""

    def __init__(self, in_channels, out_channels, trilinear=True):
        super().__init__()

        # if trilinear, use the normal convolutions to reduce the number of channels
        if trilinear:
            self.up = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)
            self.conv = DoubleConv(in_channels, out_channels, mid_channels=in_channels // 2)
        else:
            self.up = nn.ConvTranspose3d(in_channels, in_channels // 2, kernel_size=2, stride=2)
            self.conv = DoubleConv(in_channels, out_channels)


    def forward(self, x1, x2):
        x1 = self.up(x1)
        # modified for 3D input
        diffZ = x2.size()[-3] - x1.size()[-3]
        diffY = x2.size()[-2] - x1.size()[-2]
        diffX = x2.size()[-1] - x1.size()[-1]

        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,
                        diffY // 2, diffY - diffY // 2,
                        diffZ // 2, diffZ - diffZ // 2])

        # if you have padding issues, see
        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a
        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd
        x = torch.cat([x2, x1], dim=1)
        return self.conv(x)


class OutConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(OutConv, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=1)

    def forward(self, x):
        return self.conv(x)


class DepthwiseSeparableConv3d(nn.Module):
    def __init__(self, nin, nout, kernel_size, padding, kernels_per_layer=1):
        super(DepthwiseSeparableConv3d, self).__init__()
        self.depthwise = nn.Conv3d(nin, nin * kernels_per_layer, kernel_size=kernel_size, padding=padding, groups=nin)
        self.pointwise = nn.Conv3d(nin * kernels_per_layer, nout, kernel_size=1)

    def forward(self, x):
        out = self.depthwise(x)
        out = self.pointwise(out)
        return out
    
if torch.cuda.is_available():
    device = torch.device('cuda')
    print("Using CUDA")
else:
    device = torch.device('cpu')
    print("Using CPU")
model = UNet(1, 3).to(device)

start_time = time.time()
summary(model=model, input_size=(1, 19, 384, 384), batch_size=-1, device="cuda")
print("--- %s seconds ---" % (time.time() - start_time))

# !pip install monai==1.2.0

from monai.losses import DiceCELoss, DiceLoss

batch_size = 1  # change
num_epochs = 1

lr = 4.3e-3
beta1 = 0.9
beta2 = 0.95 #0.999
weight_decay = 0.05 #1e-5

optimizer = torch.optim.AdamW(model.parameters(),
                                            lr=lr,
                                            betas=(beta1, beta2),
                                            weight_decay=weight_decay)

smooth_nr = 0.0
smooth_dr = 1e-6

# to_onehot_y=True,
criterion = DiceCELoss(softmax=True,
                                    squared_pred=True,
                                    smooth_nr=smooth_nr,
                                    smooth_dr=smooth_dr)

from torch.utils.data import DataLoader, random_split

torch.manual_seed(0)

dataset_size = len(dataset)
train_size = int(0.2 * dataset_size)
val_size = int(0.2 * dataset_size)

train_dataset, _, test_dataset = random_split(dataset, [train_size, val_size, dataset_size - train_size - val_size])

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

print("Total number of training scans:")
len(train_loader.dataset)

losses = np.array([])
# val_losses = np.array([])
for epoch in range(num_epochs):
  for batch_idx, (inputs, targets) in enumerate(train_loader):
    inputs, targets = inputs.to(device).float(), targets.to(device)
    inputs = inputs[None, :, :, :, :] # use 1 input in batch but pretend it's a whole batch
    optimizer.zero_grad()

    outputs = model(inputs)
    loss = criterion(outputs.permute((0,1,3,4,2)), targets.permute((0,1,3,4,2)))
    losses = np.append(losses, loss.item())

    loss.backward()
    optimizer.step()
    break

    # with torch.no_grad():
    #   for val_batch_idx, (val_inputs, val_targets) in enumerate(val_loader):
    #     val_inputs, val_targets = val_inputs.to(device).float(), val_targets.to(device)
    #     val_inputs = val_inputs[None, :, :, :, :]

    #     val_outputs = model(val_inputs)
    #     val_loss = criterion(val_outputs.permute((0,1,3,4,2)), val_targets.permute((0,1,3,4,2)))
    #     val_losses = np.append(val_losses, val_loss.item())

    #   val_losses = np.append(val_losses, np.average(val_losses))
    # print(f"Batch {batch_idx+1}, Loss: {loss.item()}, Val: {val_loss.item()}")
    print(f"Batch {batch_idx+1}, Loss: {loss.item()}")

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.plot(losses, marker='o')
plt.xlabel('Batch')
plt.ylabel('Loss')
plt.title('DiceCELoss Curve with Improved Params')
plt.grid(True)
plt.show()

# device = torch.device('cpu')
# model = model.to(device)

loss = 0
for batch_idx, (inputs, targets) in enumerate(test_loader):
  inputs, targets = inputs.to(device).float(), targets.to(device)
  inputs = inputs[None, :, :, :, :] # use 1 input in batch but pretend it's a whole batch

  outputs = model(inputs)
  loss += criterion(outputs.permute((0,1,3,4,2)), targets.permute((0,1,3,4,2)))
  print(f"Batch {batch_idx+1}, Loss: {loss.item()}")